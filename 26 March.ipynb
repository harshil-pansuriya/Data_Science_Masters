{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a156b96-d170-4c53-947a-f66a4dcd6d87",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1.\n",
    "## Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
    "### Simple Linear Regression:\n",
    "### --> In simple linear regression, there is only one independent variable used to predict the dependent variable. It is a linear approach where a straight line is fit to the data points to model the relationship between the two variables. For example, let's say we want to model the relationship between the number of hours studied and the score on a math exam. Here, the number of hours studied is the independent variable and the score on the exam is the dependent variable. The simple linear regression model can be written as:\n",
    "### y = β0 + β1x\n",
    "\n",
    "### Multiple Linear Regression:\n",
    "### --> In multiple linear regression, there are two or more independent variables used to predict the dependent variable. For example, let's say we want to model the relationship between the price of a house and its various characteristics such as size, number of bedrooms, and location. Here, the price of the house is the dependent variable and the size, number of bedrooms, and location are the independent variables. The multiple linear regression model can be written as:\n",
    "### y = β0 + β1x1 + β2x2 + β3x3 + ... + βnxn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cedf89-57b3-4afd-bef3-07712c2e3ebd",
   "metadata": {},
   "source": [
    "# 2.\n",
    "## Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
    "### --> Assumptions of linear regression:\n",
    "\n",
    "### 1] Linearity: The relationship between the dependent variable and independent variables is linear. \n",
    "### 2] Independence: The observations are independent of each other. There is no systematic relationship between the residuals and the independent variables.\n",
    "### 3] Homoscedasticity: The variance of the residuals is constant across all levels of the independent variable.\n",
    "### 4] Normality: The residuals follow a normal distribution. This assumption is important because if the residuals are not normally distributed, the confidence intervals and p-values may not be accurate.\n",
    "### 5] No multicollinearity: The independent variables are not highly correlated with each other. Multicollinearity can make it difficult to estimate the effect of each independent variable on the dependent variable.\n",
    "### Check whether these assumptions hold in a given dataset, several diagnostic tools can be used:\n",
    "#### 1] Residual plots\n",
    "#### 2] Normal probability plot\n",
    "#### 3] Cook's distance\n",
    "#### 4] Variance inflation factor (VIF)\n",
    "#### 5] Durbin-Watson test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b6ab38-75d6-4050-9129-46d0b41dd55c",
   "metadata": {},
   "source": [
    "# 3.\n",
    "## How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
    "### Interpreting the slope:\n",
    "### --> The slope of a linear regression model indicates the change in the dependent variable for every one-unit increase in the independent variable. For example, if the slope is 2, it means that for every one-unit increase in the independent variable, the dependent variable increases by 2 units.\n",
    "### Interpreting the intercept:\n",
    "### --> The intercept represents the value of the dependent variable when the independent variable is zero. For example, if the intercept is 5, it means that when the independent variable is zero, the dependent variable has a value of 5.\n",
    "### Ex : Price = β0 + β1*Size + ε"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3254a472-cce6-4410-9f0e-272e513655eb",
   "metadata": {},
   "source": [
    "# 4.\n",
    "## Explain the concept of gradient descent. How is it used in machine learning?\n",
    "### --> Gradient descent is an iterative optimization algorithm used to find the minimum of a cost function. The cost function represents the difference between the predicted output and the actual output in a machine learning model. The goal of gradient descent is to find the set of parameters that minimize the cost function and produce the best possible predictions.\n",
    "\n",
    "### -->The basic idea of gradient descent is to start with an initial guess for the parameters, and then iteratively update the parameters in the direction of steepest descent of the cost function. The update is performed by taking the gradient  of the cost function with respect to each parameter and adjusting the parameter values proportionally to the gradient. \n",
    "### --> The gradient indicates the direction of the steepest ascent of the cost function, so multiplying it by a negative constant ensures that the updates are in the direction of the steepest descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b04fd-a7de-416a-8ca9-b46490f9a3ec",
   "metadata": {},
   "source": [
    "# 5.\n",
    "## Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
    "### --> Multiple linear regression is a statistical technique used to model the relationship between a dependent variable and two or more independent variables. It is an extension of simple linear regression, which models the relationship between a dependent variable and a single independent variable.\n",
    "\n",
    "### --> In multiple linear regression, the relationship between the dependent variable and the independent variables is assumed to be linear. The model is expressed as: y = b0 + b1x1 + b2x2 + ... + bnxn + e\n",
    "### Difference:\n",
    "### 1] Number of independent variables: Simple linear regression models the relationship between a dependent variable and a single independent variable, while multiple linear regression models the relationship between a dependent variable and two or more independent variables.\n",
    "### 2] Joint effects of multiple independent variables: Multiple linear regression allows for the joint effects of multiple independent variables to be captured in the model, while simple linear regression does not.\n",
    "### 3] Interpretation of coefficients: The interpretation of the coefficients in multiple linear regression is more complex than in simple linear regression, as each coefficient represents the change in the dependent variable for a one-unit increase in the corresponding independent variable, holding all other independent variables constant.\n",
    "### 4] Model complexity: Multiple linear regression is generally more complex than simple linear regression, as it involves estimating more parameters and may require more data to achieve adequate statistical power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5de41be-2884-429f-9b94-50e202ccaa0b",
   "metadata": {},
   "source": [
    "# 6.\n",
    "## Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
    "### --> Multicollinearity is a common issue that arises in multiple linear regression when two or more independent variables in the model are highly correlated with each other. This can cause problems because it can make it difficult to distinguish the individual effects of each independent variable on the dependent variable.\n",
    "### Multicollinearity can be detected in several ways:\n",
    "### 1] Correlation matrix: The correlation matrix can be used to identify pairs of independent variables that are highly correlated with each other. A correlation coefficient greater than 0.7 or 0.8 is often used as a threshold for identifying high levels of correlation.\n",
    "### 2] Variance Inflation Factor (VIF): The VIF is a measure of how much the variance of the estimated coefficient for an independent variable is increased due to multicollinearity. A VIF value greater than 5 or 10 is often used as an indicator of high levels of multicollinearity.\n",
    "### 3] Eigenvalues: Eigenvalues can be used to identify the presence of multicollinearity in the model. If one or more eigenvalues are close to zero, it indicates that there is high multicollinearity in the model.\n",
    "\n",
    "### Once multicollinearity is detected, there are several ways to address the issue, including:\n",
    "\n",
    "### -->Removing one or more of the highly correlated independent variables from the model.\n",
    "\n",
    "### -->Combining the highly correlated independent variables into a single variable, such as a principal component, and using that variable in the model instead.\n",
    "\n",
    "### -->Collecting additional data to increase the sample size and improve the stability of the estimates.\n",
    "\n",
    "### -->Regularization techniques, such as ridge regression and lasso regression, can also be used to address multicollinearity by adding a penalty term to the regression equation that reduces the size of the coefficients and encourages sparse solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec25822-02ab-425f-9ba5-0ec5d90e47fc",
   "metadata": {},
   "source": [
    "# 7.\n",
    "## Describe the polynomial regression model. How is it different from linear regression?\n",
    "\n",
    "### --> Polynomial regression is a type of regression analysis in which the relationship between the independent variable and the dependent variable is modeled as an nth degree polynomial function. Unlike linear regression, which models a linear relationship between the variables, polynomial regression allows for more complex, nonlinear relationships to be modeled.\n",
    "### --> In a polynomial regression model, the equation takes the form:y = β0 + β1x + β2x^2 + ... + βnx^n + ε\n",
    "### --> The key difference between polynomial regression and linear regression is that in polynomial regression, the regression equation includes additional higher-order terms, such as x^2, x^3, and so on, in addition to the linear term (x). This allows for more flexibility in modeling the relationship between the variables.\n",
    "### --> The key difference between polynomial regression and linear regression is that polynomial regression can capture nonlinear relationships between the variables, while linear regression assumes a linear relationship between the variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8cc482-24a7-4acd-8b14-5537f0db5f4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 8.\n",
    "## What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
    "### Advantages\n",
    "### -> Flexibility: Polynomial regression can model more complex, nonlinear relationships between the independent variable and dependent variable than linear regression.\n",
    "### -> Improved fit: Polynomial regression can fit the data better than linear regression if the relationship between the variables is nonlinear.\n",
    "### -> Extrapolation: Polynomial regression can be used to extrapolate beyond the range of the independent variable in the dataset, which is not possible with linear regression.\n",
    "\n",
    "### Disadvantages \n",
    "### -> Overfitting: With too many polynomial terms, the model can become too complex and overfit the data, meaning it will fit the noise in the data rather than the underlying signal.\n",
    "### -> Difficulty in interpretation: With multiple polynomial terms, the interpretation of the coefficients becomes more difficult.\n",
    "### -> Extrapolation: While extrapolation can be an advantage, it can also be a disadvantage because it is uncertain how well the model will perform outside the range of the independent variable in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26005761-5f36-4b5f-8388-f31382cb9559",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
