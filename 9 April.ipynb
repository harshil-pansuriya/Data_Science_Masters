{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329d0137-40bb-4004-bfad-cf5b28ddec0f",
   "metadata": {},
   "source": [
    "# 1.\n",
    "## What is Bayes' theorem?\n",
    "#### ->Bayes' theorem, named after the Reverend Thomas Bayes, is a fundamental concept in probability theory and statistics. It provides a way to update our beliefs or knowledge about an event or hypothesis based on new evidence or information.\n",
    "### The theorem mathematically describes the relationship between two conditional probabilities: the probability of event A given event B and the probability of event B given event A. It can be stated as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7758369d-1997-4cf1-bbec-0e05f9dcba3f",
   "metadata": {},
   "source": [
    "# 2.\n",
    "## What is the formula for Bayes' theorem?\n",
    "#### P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "#### Where:\n",
    "#### P(A|B) represents the probability of event A occurring given that event B has already occurred.\n",
    "#### P(B|A) represents the probability of event B occurring given that event A has already occurred.\n",
    "#### P(A) is the prior probability of event A, which is our initial belief in the absence of any evidence.\n",
    "#### P(B) is the prior probability of event B, which is our initial belief in the absence of any evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10080ab3-4eb1-46e7-9d9c-b6c5164c63f0",
   "metadata": {},
   "source": [
    "# 3.\n",
    "## How is Bayes' theorem used in practice?\n",
    "### ->Bayes' theorem is widely used in practice across various fields for reasoning under uncertainty and making probabilistic predictions or decisions. Here are a few examples of how Bayes' theorem is applied:\n",
    "\n",
    "####  1] Bayesian Inference: Bayes' theorem is fundamental to Bayesian inference, a statistical framework that combines prior knowledge or beliefs with observed data to obtain updated or posterior probabilities. It allows for iterative learning and updating of probabilities as new evidence becomes available. Bayesian inference is used in fields such as data analysis, machine learning, and AI to make predictions, estimate parameters, and perform statistical modeling.\n",
    "\n",
    "####  2] Medical Diagnosis: Bayes' theorem is applied in medical diagnosis to assess the likelihood of a particular disease given observed symptoms or test results. Prior probabilities, representing the prevalence of the disease in a population, are combined with the likelihoods of observed symptoms given the disease (obtained from medical literature or data) to compute the posterior probability of having the disease. This helps doctors make informed decisions about diagnoses and treatment options.\n",
    "\n",
    "####  3] Spam Filtering: Bayes' theorem is utilized in spam filtering algorithms. It involves building a probabilistic model based on a training dataset containing labeled emails (spam or non-spam). The algorithm calculates the probability that an email is spam given certain words or features in the email. Bayes' theorem is used to update the probabilities based on observed features in an incoming email, enabling accurate classification of spam and non-spam emails.\n",
    "\n",
    "####  4] Document Classification: Bayes' theorem is employed in text document classification tasks. It helps determine the probability that a given document belongs to a particular category (e.g., sports, politics, technology) based on the frequencies or occurrences of words in the document. By applying Bayes' theorem, the classification model can update the probabilities of document-category associations and make predictions about the category of unseen documents.\n",
    "\n",
    "####  5] Fault Diagnosis: Bayes' theorem is used in fault diagnosis and reliability analysis. It helps assess the probability of a particular component or system failure given observed symptoms or indicators. Prior probabilities, based on historical failure rates or expert knowledge, are combined with conditional probabilities of observed symptoms given the failure to compute the posterior probability of the failure. This aids in identifying and addressing potential faults in complex systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2ee82b-4fa8-4b6a-b8e4-1777f1db6502",
   "metadata": {},
   "source": [
    "# 4.\n",
    "## What is the relationship between Bayes' theorem and conditional probability?\n",
    "### -> Bayes' theorem and conditional probability are closely related concepts. In fact, Bayes' theorem is a mathematical expression that describes the relationship between two conditional probabilities.\n",
    "### -> Conditional probability is the probability of an event A occurring given that another event B has already occurred, and it is denoted as P(A|B). It represents the likelihood of event A happening under the condition that event B is true.\n",
    "### -> Bayes' theorem provides a way to calculate the posterior probability of event A given event B, by relating it to the conditional probability of event B given event A and the prior probabilities of events A and B. The theorem can be stated as:\n",
    "#### P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "### -> In this formula, P(B|A) is the conditional probability of event B occurring given that event A has already occurred. P(A) is the prior probability of event A, which is our initial belief in the absence of any evidence. P(B) is the prior probability of event B, which is our initial belief in the absence of any evidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2274e205-8e60-479c-8178-9f3778b6f651",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 5.\n",
    "## How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
    "### When choosing a type of Naive Bayes classifier for a given problem, the decision typically depends on the specific characteristics and requirements of the problem at hand. Here are some considerations to guide the selection process:\n",
    "### 1]Multinomial Naive Bayes:\n",
    "#### Feature Representation: Multinomial Naive Bayes is commonly used when dealing with discrete features, such as word counts or frequencies in text classification problems.\n",
    "#### Feature Independence Assumption: It assumes that the features are generated from a multinomial distribution, making it suitable for problems where the occurrence or frequency of different features matters.\n",
    "### 2]Gaussian Naive Bayes:\n",
    "#### Feature Representation: Gaussian Naive Bayes is appropriate when dealing with continuous features that can be represented by a Gaussian (normal) distribution.\n",
    "#### Feature Independence Assumption: It assumes that the features are conditionally independent given the class labels. This assumption may not hold in all cases, but Gaussian Naive Bayes can still perform reasonably well if the dependence is not significant.\n",
    "### 2]Bernoulli Naive Bayes:\n",
    "#### Feature Representation: Bernoulli Naive Bayes is useful when working with binary or Boolean features, where each feature can take only two possible values (e.g., presence or absence of a feature).\n",
    "#### Feature Independence Assumption: It assumes that the features are conditionally independent and follow a Bernoulli distribution. This classifier is often used in text classification tasks, where the presence or absence of specific words is considered.\n",
    "### 4]Complement Naive Bayes:\n",
    "#### Class Imbalance: Complement Naive Bayes is designed to handle class-imbalanced datasets, where the majority class dominates the training data. It can be effective when the minority class is of particular interest, as it tries to compensate for the skewed class distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d60d0d-e365-4bf5-8f4e-a5fe7349f7fa",
   "metadata": {},
   "source": [
    "# 6.\n",
    "## Assignment:\n",
    "#### You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "#### Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "#### each feature value for each class:\n",
    "#### Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "#### A 3 3 4 4 3 3 3\n",
    "#### B 2 2 1 2 2 2 3\n",
    "#### Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance to belong to?\n",
    "\n",
    "#### ->To predict the class of a new instance using Naive Bayes, we need to calculate the conditional probabilities of each class given the feature values and then select the class with the highest probability.\n",
    "#### -> Let's calculate the conditional probabilities for the given dataset:\n",
    " ### Class A:                         ---------------->Class B:\n",
    " ### P(X1=3|A)=4/10=0.4               ----> P(X1=3|B)=1/7≈0.143\n",
    " ### P(X2=4|A)=3/10=0.3               ---->P(X2=4|B)=3/7≈0.429\n",
    "\n",
    "#### Since we assume equal prior probabilities for each class, we don't need to consider the prior probabilities in this case\n",
    "#### Next, we calculate the likelihood of the new instance belonging to each class:\n",
    "#### Likelihood for Class A:  P(X1 = 3, X2 = 4 | A) = P(X1 = 3 | A) * P(X2 = 4 | A) = 0.4 * 0.3 = 0.12\n",
    "#### Likelihood for Class B: P(X1 = 3, X2 = 4 | B) = P(X1 = 3 | B) * P(X2 = 4 | B) = 0.143 * 0.429 ≈ 0.061"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb82bc-4342-4347-b6db-bd8d307e98da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
