{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be645821-7839-4085-b01c-03b808237502",
   "metadata": {},
   "source": [
    "# 1.\n",
    "## What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.'\n",
    "### Min-Max scaling, also known as normalization, is a data preprocessing technique used to transform features or variables in a dataset to a specific range, typically [0, 1]. The goal of Min-Max scaling is to bring all the features to a similar scale, which helps in preventing one feature from dominating others in terms of magnitude and allows for fair comparisons between features.\n",
    "### X_scaled = (X - X_min) / (X_max - X_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a199181a-603f-4500-a5eb-5579a4fe686f",
   "metadata": {},
   "source": [
    "# 2.\n",
    "## What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "#### -->The Unit Vector technique, also known as vector normalization, is a data preprocessing technique used to scale features or variables in a dataset to have a unit magnitude or length. In other words, it transforms the features to lie on the surface of a unit hypersphere with a radius of 1. The Unit Vector technique is commonly used in machine learning algorithms that rely on distance-based calculations, such as K-nearest neighbors (KNN) and support vector machines (SVM).\n",
    "#### The formula for Unit Vector scaling is as follows:\n",
    "#### X_scaled= x/|x|\n",
    "#### -->The difference between Unit vector scaling and Min-Max scaling is that the former rescales the feature vectors to have unit length, while the latter rescales the feature values to a specific range between 0 and 1. Unit vector scaling preserves the direction of the original feature vectors, while Min-Max scaling changes their direction in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1923b2ff-b758-42d8-90f2-cf1bcac1fa7a",
   "metadata": {},
   "source": [
    "# 3.\n",
    "## What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "### --?Principal Component Analysis (PCA) is a statistical technique used in machine learning and data analysis to reduce the dimensionality of a dataset. It involves transforming a high-dimensional dataset into a lower-dimensional representation while retaining the most important information or patterns in the data. PCA achieves this by creating a set of new, orthogonal (uncorrelated) variables called principal components (PCs) that are linear combinations of the original features.\n",
    "### --> PCA is commonly used in dimensionality reduction, where the goal is to reduce the number of features in a dataset while retaining the most important information. By projecting the data onto a lower-dimensional subspace defined by the principal components, PCA can help to reduce noise, remove redundant or irrelevant features, and simplify the analysis or visualization of the data.\n",
    "### --> Suppose we have a dataset containing the following variables: height, weight, age, and income. We want to reduce the dimensionality of this dataset while retaining as much information as possible.\n",
    "### --> First, we standardize the data to have a mean of zero and a standard deviation of one. Then, we use PCA to find the principal components of the dataset. Let's say that the first two principal components capture 90% of the variation in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3078132b-05eb-4f13-b496-9769c088bade",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 4.\n",
    "## What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "#### --> Feature Extraction is the process of selecting and transforming a set of input features into a new set of features that better represents the underlying patterns in the data. It is a crucial step in many machine learning and data analysis tasks, such as classification, clustering, and visualization.\n",
    "#### --> PCA can be used for Feature Extraction by finding the principal components of a dataset and using them as the new set of features. The principal components represent the directions of maximum variance in the data and capture the most important patterns or structures in the data.\n",
    "#### --> Suppose we have a dataset containing images of handwritten digits, and each image is represented as a vector of pixel intensities. We want to classify these images into different digits, but the high dimensionality of the data makes it difficult to do so.\n",
    "#### --> We can then use the first few principal components as the new set of features and train a classifier on this reduced set of features. By using PCA for Feature Extraction, we are able to reduce the dimensionality of the data and capture the most important patterns, which can improve the accuracy and efficiency of the classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95989bb1-62d5-4b7e-80ee-52fc4e03acd9",
   "metadata": {},
   "source": [
    "# 5.\n",
    "## You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "### --> Min-Max scaling is a common technique used in data preprocessing to scale numerical features within a specific range, typically between 0 and 1. \n",
    "### Original dataset:\n",
    "### Item\tPrice (Scaled)\tRating\tDelivery Time\n",
    "### Item 1\t  0.294\t         4.5\t30 mins\n",
    "### Item 2\t  0.085\t         3.8\t25 mins\n",
    "### Item 3\t  0.537\t         4.2\t40 mins\n",
    "\n",
    "### 1.Identify the feature to be scaled: In this case, the \"Price\" feature needs to be scaled.\n",
    "\n",
    "### 2.Determine the minimum and maximum values: After computing the minimum and maximum values of the \"Price\" feature in the dataset, let's say we get min_value = $8.00 and max_value = $20.00.\n",
    "\n",
    "### 3.Apply Min-Max scaling formula: Using the Min-Max scaling formula, we can scale the \"Price\" feature as follows:\n",
    "\n",
    "### 4.Update the dataset with scaled values: Replace the original \"Price\" values in the dataset with their corresponding scaled values obtained from the Min-Max scaling formula.\n",
    "### Updated dataset:\n",
    "###  Item\t Price (Scaled)\tRating\t Delivery Time\n",
    "### Item 1\t  0.294\t         4.5\t 30 mins\n",
    "### Item 2\t  0.085\t         3.8\t 25 mins\n",
    "### Item 3\t  0.537\t         4.2\t 40 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c617553-b198-4c12-9a63-76f1994d35a8",
   "metadata": {},
   "source": [
    "# 6.\n",
    "## You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "### PCA (Principal Component Analysis) is a technique commonly used for dimensionality reduction in machine learning projects, including stock price prediction.\n",
    "#### 1]Dataset: Your dataset contains financial data of 1000 companies, with 30 features each, such as revenue, expenses, net income, stock indices, interest rates, and economic indicators, resulting in a high-dimensional dataset.\n",
    "#### 2]Preprocess the dataset: You perform data preprocessing tasks such as handling missing values, normalizing the data to bring all features to a similar scale, and encoding categorical variables.\n",
    "#### 3]Perform PCA: You apply PCA on the preprocessed dataset to reduce its dimensionality. PCA identifies the principal components that explain the maximum variance in the dataset.\n",
    "#### 4]Select the number of principal components: You analyze the scree plot and determine that the first 10 principal components explain 80% of the total variance in the dataset, so you decide to retain only these 10 principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d0fdf8-a159-4d34-bc7f-95c7c226d2b1",
   "metadata": {},
   "source": [
    "# 7.\n",
    "## For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "####  The Min-Max scaled values for the given dataset, transformed to a range of -1 to 1, are [-1, -0.5789, -0.0526, 0.4737, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9ccc5-b8b1-462c-9110-375d0aa7fb04",
   "metadata": {},
   "source": [
    "# 8.\n",
    "## For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "### --> When performing feature extraction using Principal Component Analysis (PCA) on a dataset with features [height, weight, age, gender, blood pressure], the number of principal components to retain would depend on the specific characteristics of the dataset and the goals of the analysis.\n",
    "### Steps top perform PCA on data\n",
    "#### Standardize the data\n",
    "#### Compute covariance matrix or correlation matrix\n",
    "#### Compute eigenvectors and eigenvalues\n",
    "#### Choose the principal components\n",
    "#### Transform the data\n",
    "### Ultimately, the choice of the number of principal components to retain in PCA would depend on the specific context of your project, the goals of your analysis, and any trade-offs between interpretability, model performance, and dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09387794-fe61-4b4a-a8e6-f99dad4fc4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
