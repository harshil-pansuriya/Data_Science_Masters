{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684cca75-e025-4e47-b9ac-e0fd381cdeee",
   "metadata": {},
   "source": [
    "# 1.\n",
    "## What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    "### ->Polynomial functions and kernel functions have a close relationship in machine learning algorithms, particularly in kernel methods such as Support Vector Machines (SVMs).\n",
    "\n",
    "### ->A polynomial function is a mathematical function that consists of one or more terms, with each term being a constant multiplied by one or more variables raised to non-negative integer exponents. Polynomial functions can capture nonlinear relationships between input features.\n",
    "\n",
    "### ->In machine learning, kernel functions are used to implicitly map the input features into a higher-dimensional feature space, where linear separation is possible. Kernel functions provide a way to compute dot products between feature vectors in the higher-dimensional space without explicitly transforming the data. They allow nonlinear relationships to be captured in a computationally efficient manner.\n",
    "\n",
    "### ->Now, the relationship between polynomial functions and kernel functions arises from the fact that certain kernel functions can be interpreted as implicitly representing a polynomial function in a higher-dimensional space.\n",
    "\n",
    "### ->The polynomial kernel function is a type of kernel function that computes the dot product between two vectors as if they were mapped into a higher-dimensional space using a polynomial function. It enables SVMs to learn nonlinear decision boundaries by implicitly working with polynomial functions of the input features.\n",
    "\n",
    "### ->The polynomial kernel function is defined as:\n",
    "#### K(x, y) = (gamma * <x, y> + coef0)^degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9069fa0-598f-4f01-81c6-c12b2cbf9fbd",
   "metadata": {},
   "source": [
    "# 2.\n",
    "## How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e37d0cc-6ae6-4f6f-bf85-c60b65e1eeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.885\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X,y =make_classification(n_samples=1000,n_features=5,n_redundant=2,shuffle=True,random_state=None)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "clf = SVC(kernel='poly', degree=3, random_state=42)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred=clf.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b1007c-4aae-48c2-be35-82867ff5f3a5",
   "metadata": {},
   "source": [
    "# 3.\n",
    "## How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "### ->In Support Vector Regression (SVR), the parameter epsilon (ε) determines the width of the epsilon-insensitive tube around the regression line. It controls the trade-off between the model's accuracy and the number of support vectors.\n",
    "### ->Support vectors are the data points that lie on the boundaries of the epsilon-insensitive tube or contribute to defining the regression line. They have a non-zero dual coefficient value (α) in the solution of the optimization problem.\n",
    "### ->When you increase the value of epsilon (ε), the width of the epsilon-insensitive tube becomes larger. As a result, more data points may fall within or close to the tube, leading to a higher chance of being classified as support vectors.\n",
    "### ->In other words, increasing epsilon allows more data points to be within the acceptable margin of error. Consequently, the number of support vectors is likely to increase when epsilon is larger."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06faae8-07c9-4c64-9ab8-36d03994d1be",
   "metadata": {},
   "source": [
    "# 4.\n",
    "## How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "### ->In Support Vector Regression (SVR), the choice of kernel function, C parameter, epsilon parameter, and gamma parameter can significantly impact the performance and behavior of the model. Let's explore each parameter and its effect:\n",
    "### 1] Kernel function:The kernel function determines the type of non-linear mapping used to transform the input features into a higher-dimensional space. Common kernel functions include linear, polynomial, radial basis function (RBF), and sigmoid. Each kernel has different characteristics and is suitable for different types of data and relationships. Here are some considerations:\n",
    "#### Linear kernel: Suitable for linearly separable data without complex patterns.\n",
    "#### Polynomial kernel: Useful when data has non-linear patterns with different degrees of complexity. The degree parameter controls the degree of the polynomial.\n",
    "#### RBF kernel: Effective when data has non-linear patterns and the degree of complexity is not known. The gamma parameter controls the width of the Gaussian function.\n",
    "#### Sigmoid kernel: Appropriate for data with non-linear patterns resembling sigmoid functions.\n",
    "### 2] C parameter (Regularization parameter):The C parameter determines the trade-off between model complexity and training error. It controls the penalty for misclassifying training examples. Considerations for the C parameter:\n",
    "#### Smaller C: Allows more misclassifications and creates a larger margin, leading to a simpler model with potentially higher bias but lower variance. It generalizes better but might sacrifice accuracy.\n",
    "#### Larger C: Emphasizes the importance of classifying training examples correctly, resulting in a smaller margin and a potentially more complex model. It can lead to lower bias but higher variance. It tends to fit the training data more closely.\n",
    "### 3] Epsilon parameter:The epsilon parameter (ε) defines the width of the epsilon-insensitive tube around the regression line in SVR. It determines the acceptable margin of error for points within the tube. Considerations for the epsilon parameter:\n",
    "#### Smaller epsilon: Constrains the allowable margin of error tightly. It results in a more sensitive model that tries to fit the data precisely. It might lead to overfitting if the noise level is high.\n",
    "#### Larger epsilon: Allows a wider margin of error, resulting in a more lenient model that tolerates higher deviations. It may produce a more generalizable model and be less affected by noise.\n",
    "### 4] Gamma parameter:The gamma parameter defines the influence of each training example on the decision boundary. It affects the flexibility of the model. Considerations for the gamma parameter:\n",
    "#### Smaller gamma: Considers a broader influence range for each training example. It tends to create smoother decision boundaries and can generalize better. It is suitable for large datasets.\n",
    "#### Larger gamma: Considers a smaller influence range for each training example. It leads to more complex and intricate decision boundaries, potentially overfitting the training data. It is suitable for smaller datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b492b59-d96d-4d40-9e9e-fd2796e1b3b0",
   "metadata": {},
   "source": [
    "# 5.\n",
    "## Assignment:\n",
    "####  Import the necessary libraries and load the dataset\n",
    "####  Split the dataset into training and testing sets\n",
    "####  Preprocess the data using any technique of your choice (e.g. scaling, normaliMation)\n",
    "####  Create an instance of the SVC classifier and train it on the training data\n",
    "####  Use the trained classifier to predict the labels of the testing data\n",
    "####  Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy, precision, recall, F1-scoreK\n",
    "####  Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to improve its performance\n",
    "####  Train the tuned classifier on the entire dataset\n",
    "####  Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111f4ec6-010c-40e6-84e5-cb3d875d5060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8055555555555556\n",
      "[[14  0  0]\n",
      " [ 0 11  3]\n",
      " [ 0  4  4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        14\n",
      "           1       0.73      0.79      0.76        14\n",
      "           2       0.57      0.50      0.53         8\n",
      "\n",
      "    accuracy                           0.81        36\n",
      "   macro avg       0.77      0.76      0.76        36\n",
      "weighted avg       0.80      0.81      0.80        36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n",
    "wine=load_wine()\n",
    "X=wine.data\n",
    "y=wine.target\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "\n",
    "svm=SVC()\n",
    "svm.fit(X_train,y_train)\n",
    "\n",
    "y_pred=svm.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bf022fe-2564-43d1-b883-f6cc866d8a99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [100, 10, 1, 0.1, 0.01],\n",
       "                         &#x27;kernel&#x27;: [&#x27;poly&#x27;, &#x27;rbf&#x27;, &#x27;linear&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={&#x27;C&#x27;: [100, 10, 1, 0.1, 0.01],\n",
       "                         &#x27;kernel&#x27;: [&#x27;poly&#x27;, &#x27;rbf&#x27;, &#x27;linear&#x27;]},\n",
       "             scoring=&#x27;accuracy&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(),\n",
       "             param_grid={'C': [100, 10, 1, 0.1, 0.01],\n",
       "                         'kernel': ['poly', 'rbf', 'linear']},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "classifier=SVC()\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=42)\n",
    "\n",
    "parameter={\n",
    "    \"C\":[100,10,1,0.1,0.01],\n",
    "    \"kernel\":[\"poly\",\"rbf\",\"linear\"],\n",
    "}\n",
    "clf=GridSearchCV(classifier,param_grid=parameter,cv=5,scoring='accuracy')\n",
    "clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcded505-fbde-4fec-a171-e2eb6d14695c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fef12db-951c-4b2e-bde7-f9fba96be71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e317cb0-9eb0-45d9-84f8-9f97b21c4fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "with open('svc_assignment.pkl', 'wb') as f:\n",
    "    pkl.dump(clf, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a4da5-b7a9-4f69-841f-eb30c4c9b218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
