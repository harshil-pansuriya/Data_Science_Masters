{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d139480-6bf3-4d09-a058-c8e396eea6a5",
   "metadata": {},
   "source": [
    "# 1.\n",
    "## What is anomaly detection and what is its purpose?\n",
    "### --> Anomaly detection, also known as outlier detection, is a technique used in data analysis to identify instances that deviate significantly from the expected or normal behavior within a dataset. Anomalies are data points that do not conform to the majority of the data points, showing patterns that are different, unusual, or potentially indicative of errors, fraud, defects, or other interesting phenomena.\n",
    "\n",
    "### --> The purpose of anomaly detection is to uncover instances that stand out from the norm, often indicating events or conditions that require special attention or investigation. Anomalies can have various implications depending on the context of the data:\n",
    "\n",
    "#### 1] Error Detection\n",
    "#### 2] Fraud Detection\n",
    "#### 3] Quality Control\n",
    "#### 4] Security\n",
    "#### 5] Healthcare\n",
    "#### 6] Predictive Maintenance\n",
    "#### 7] Environmental Monitoring\n",
    "#### 8] Natural Phenomena"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0546e9ae-9f3f-4d87-b7e1-a3856545df16",
   "metadata": {},
   "source": [
    "# 2.\n",
    "## What are the key challenges in anomaly detection?\n",
    "#### 1] Anomaly detection comes with several challenges that need to be addressed to build effective and reliable anomaly detection systems. Some of the key challenges include:\n",
    "#### 2] Unbalanced Data: Anomalies are often rare compared to normal instances, leading to imbalanced datasets. This can affect the performance of traditional machine learning algorithms that are biased toward the majority class.\n",
    "#### 3] Feature Selection: Identifying relevant features that effectively capture the differences between normal and anomalous instances is crucial. Poor feature selection can lead to reduced accuracy in detecting anomalies.\n",
    "#### 4] Changing Patterns: Anomalies can evolve over time, and the detection model needs to adapt to new patterns and behaviors. This requires continuous monitoring and updating of the model.\n",
    "#### 5] Lack of Labeled Anomalies: Supervised anomaly detection relies on labeled anomalous data for training. However, obtaining accurate and sufficient labeled anomalies can be challenging and expensive.\n",
    "#### 6] High-Dimensional Data: In high-dimensional feature spaces, defining what constitutes an anomaly becomes complex. High-dimensional data can also lead to the \"curse of dimensionality,\" affecting the performance of some algorithms.\n",
    "#### 7] Noise in Data: Data often contain noise or errors, which can lead to false positives (normal instances classified as anomalies) or false negatives (anomalies classified as normal instances).\n",
    "#### 8] Interpreting Anomalies: Understanding the reasons behind an instance being labeled as an anomaly is essential for making informed decisions. Lack of interpretability can limit the adoption of anomaly detection in some applications.\n",
    "#### 9] Scalability: As data volumes grow, the scalability of anomaly detection algorithms becomes a challenge. Some algorithms might struggle to handle large datasets efficiently.\n",
    "#### 10] Threshold Setting: Setting appropriate thresholds for anomaly detection can be difficult. Too strict a threshold might lead to missed anomalies, while too lenient a threshold might result in false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c860aee-8f00-4f44-a61d-42a2b0ca126a",
   "metadata": {},
   "source": [
    "# 3.\n",
    "## How does unsupervised anomaly detection differ from supervised anomaly detection?\n",
    "### --> Unsupervised anomaly detection and supervised anomaly detection are two different approaches to identifying anomalies in a dataset, each with its own characteristics and requirements:\n",
    "\n",
    "#### Key Differences\n",
    "#### 1] Training Data: Unsupervised methods require only an unlabeled dataset, while supervised methods need a labeled dataset for training.\n",
    "#### 2] Label Dependency: Unsupervised methods do not depend on labeled anomalies for training; they infer anomalies based on the structure of the data. Supervised methods rely on labeled anomalies to learn the distinctions between normal and anomalous instances.\n",
    "#### 3] Applicability: Unsupervised methods are suitable when labeled anomalies are scarce, expensive, or unavailable. Supervised methods are useful when labeled anomalies are readily accessible and the goal is to create a precise anomaly detection model.\n",
    "#### 4] Model Complexity: Supervised methods often involve more complex models like decision trees, neural networks, etc., as they aim for higher precision. Unsupervised methods can be simpler and focus on finding data structures.\n",
    "#### 5] Human Intervention: Unsupervised methods require less human intervention in terms of labeling anomalies. Supervised methods require manual labeling of anomalies during the training phase.\n",
    "#### 6] Adaptation to New Anomalies: Unsupervised methods can more easily adapt to new types of anomalies that were not present in the training data. Supervised methods might struggle to detect new types of anomalies if they were not seen during training.\n",
    "#### 7] False Positives/Negatives: Unsupervised methods might have more false positives due to the absence of labeled anomalies. Supervised methods, if trained on a well-labeled dataset, might have fewer false positives and negatives.\n",
    "#### 8] Anomaly Interpretation: Unsupervised methods might provide less contextual information about why a particular instance is an anomaly. Supervised methods can potentially offer more interpretability based on the features that contribute to anomaly classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c89c0-df72-4e3d-88e5-6072258f8fb6",
   "metadata": {},
   "source": [
    "# 4.\n",
    "## What are the main categories of anomaly detection algorithms?\n",
    "### 1] Statistical Methods:\n",
    "#### Z-Score (Standard Score): This method measures how many standard deviations an instance is away from the mean. Instances that fall far from the mean are considered anomalies.\n",
    "#### Modified Z-Score: Similar to the standard Z-Score, but it uses the median and median absolute deviation for robustness against outliers.\n",
    "#### Gaussian Mixture Models (GMM): GMM assumes that data is generated from a mixture of several Gaussian distributions. Instances with low probability under the GMM are treated as anomalies.\n",
    "\n",
    "### 2]Machine Learning Algorithms:\n",
    "#### Supervised Learning: In supervised settings, algorithms are trained on labeled data, and instances that deviate significantly from the learned patterns are considered anomalies. However, labeled anomaly data can be scarce and expensive to obtain.\n",
    "#### Unsupervised Learning: In unsupervised settings, algorithms identify anomalies by finding patterns that deviate from the majority of the data. Clustering and density-based methods fall under this category.\n",
    "#### Semi-Supervised Learning: This approach combines aspects of both supervised and unsupervised learning, utilizing a small amount of labeled data along with a larger amount of unlabeled data.\n",
    " \n",
    "### 3]Density-Based Methods:\n",
    "#### DBSCAN (Density-Based Spatial Clustering of Applications with Noise): DBSCAN clusters data based on density and considers instances in low-density areas as anomalies.\n",
    "#### LOF (Local Outlier Factor): LOF measures the density around an instance compared to its neighbors, identifying instances with significantly lower density as anomalies.\n",
    " \n",
    "### 4]Proximity-Based Methods:\n",
    "#### K-Nearest Neighbors (KNN): KNN identifies anomalies by measuring the distance of an instance to its k-nearest neighbors. Instances with unusually high distances are considered anomalies.\n",
    "#### Distance-based Clustering: This approach involves identifying clusters of data and considering instances that do not belong to any cluster or are far from their assigned clusters as anomalies.\n",
    "\n",
    "### 5]Information-Theoretic Methods:\n",
    "#### Entropy-Based Methods: These methods analyze the entropy or information gain of attributes in a dataset to identify instances with attributes that have significantly different distributions compared to the majority.\n",
    " \n",
    "### 6] Model-Based Methods:\n",
    "#### Autoencoders: Autoencoders are neural network architectures used for dimensionality reduction and reconstruction. Instances that are poorly reconstructed are considered anomalies.\n",
    "#### One-Class SVM (Support Vector Machine): This method aims to create a boundary that encompasses the majority of data and identifies instances outside this boundary as anomalies.\n",
    "\n",
    "### 7] Time-Series Anomaly Detection:\n",
    "#### ARIMA (AutoRegressive Integrated Moving Average): ARIMA models capture temporal dependencies and deviations from expected patterns in time-series data.\n",
    "#### Seasonal Decomposition: This method decomposes time-series data into seasonal, trend, and residual components, allowing the detection of anomalies in the residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e175625-0554-4559-83bd-6d090165bf1b",
   "metadata": {},
   "source": [
    "# 5.\n",
    "## What are the main assumptions made by distance-based anomaly detection methods?\n",
    "#### 1] Normality Assumption: Distance-based methods often assume that the majority of data points in the dataset represent the \"normal\" behavior or pattern. Anomalies are considered to be instances that significantly deviate from this normal behavior.\n",
    "#### 2] Proximity Assumption: These methods assume that similar data points tend to cluster together in the feature space. Anomalies are expected to have greater distances to their nearest neighbors or cluster centers compared to normal data points.\n",
    "#### 3] Local Density Assumption: Some distance-based methods, like Local Outlier Factor (LOF), assume that anomalies are surrounded by areas of lower data density. This means that anomalies have fewer neighboring data points within a certain radius.\n",
    "#### 4] Neighborhood Consistency Assumption: In methods like k-nearest neighbors (KNN), the assumption is that normal instances have consistent or similar neighbors, while anomalies have neighbors that differ from the majority.\n",
    "#### 5] Distance Metric Assumption: The choice of distance metric is crucial in distance-based methods. Common distance metrics include Euclidean distance, Manhattan distance, and cosine similarity. The assumption is that the chosen distance metric accurately captures the similarity or dissimilarity between data points.\n",
    "#### 6] Uniform Data Distribution Assumption: Some methods assume that data points are uniformly distributed across the feature space, making it easier to determine anomalies based on deviations from this uniformity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e387d638-f1fc-48ce-9f4d-d0ce2838b2e7",
   "metadata": {},
   "source": [
    "# 6.\n",
    "## How does the LOF algorithm compute anomaly scores?\n",
    "### -->  The Local Outlier Factor (LOF) algorithm computes anomaly scores for each data point in a dataset by assessing the local density of a point with respect to its neighbors. The basic idea behind LOF is to identify instances that have significantly lower local density than their neighbors, as these instances are likely to be anomalies. Here's how LOF computes anomaly scores:\n",
    "#### 1] Compute k-Distance: For each data point, LOF calculates its k-distance, which is the distance to its k-th nearest neighbor. The value of k is a parameter set by the user.\n",
    "#### 2] Compute Reachability Distance:For each data point, the reachability distance to its k-th nearest neighbor is calculated. The reachability distance quantifies how easily a data point can be reached from its neighbors. It's essentially the maximum of the distance to the k-th nearest neighbor and the k-distance of the data point itself.\n",
    "#### 3] Calculate Local Reachability Density (LRD):The Local Reachability Density (LRD) measures the density of a data point with respect to its neighbors. It's calculated by considering the average reachability distance of the data point's neighbors. The LRD value reflects how densely the neighbors are located around the data point.\n",
    "#### 4] Compute LOF:The Local Outlier Factor (LOF) for a data point is computed by comparing its LRD to the LRD values of its k-nearest neighbors. The LOF value is the average ratio of the LRD of the data point to the LRD values of its neighbors. A LOF value significantly greater than 1 indicates that the data point has lower density compared to its neighbors and is therefore an anomaly.\n",
    "#### 5] Anomaly Score:The anomaly score for a data point can be simply the LOF value. Larger LOF values indicate a higher likelihood of the data point being an anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05767fb-e6b1-4298-af38-efd325be54d0",
   "metadata": {},
   "source": [
    "# 7.\n",
    "## What are the key parameters of the Isolation Forest algorithm?\n",
    "#### 1] n_estimators:This parameter specifies the number of isolation trees to create in the forest. Increasing the number of trees generally improves the accuracy of anomaly detection, but it also increases computational complexity.\n",
    "#### 2] max_samples:It determines the number of samples to be used for building each isolation tree. It can be an integer value or a float value between 0 and 1. When an integer is provided, it represents the exact number of samples to be used. When a float value is used, it represents the fraction of total samples to be used. Smaller values increase the randomness of the algorithm, potentially leading to better anomaly detection.\n",
    "#### 3] max_features:This parameter controls the number of features to consider for splitting at each node of an isolation tree. It can be an integer value or a float value between 0 and 1. When an integer is provided, it represents the exact number of features to be considered. When a float value is used, it represents the fraction of total features to be considered. Larger values increase the diversity of splits and randomness.\n",
    "#### 4] contamination:Contamination represents the expected proportion of anomalies in the dataset. It helps in setting a threshold for classifying instances as anomalies. For example, if the contamination is set to 0.1, the algorithm will classify the top 10% of instances with the highest anomaly scores as anomalies.\n",
    "#### 5] bootstrap:This parameter controls whether or not to use bootstrapping when selecting samples for building each isolation tree. Bootstrapping introduces additional randomness by allowing samples to be selected with replacement, making the algorithm more robust.\n",
    "#### 6] random_state:The random_state parameter sets the seed for the random number generator. Providing a specific value ensures reproducibility of results across different runs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4825adb-8fac-4704-8349-3bb776e04f85",
   "metadata": {},
   "source": [
    "# 8.\n",
    "## If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?\n",
    "### --> In the k-nearest neighbors (KNN) algorithm for anomaly detection, the anomaly score of a data point is often calculated based on the distances to its k-nearest neighbors. However, in your scenario, you mention that the data point has only 2 neighbors of the same class within a radius of 0.5. \n",
    "### -->In this case, the KNN algorithm would not be able to find 10 neighbors since there are only 2 neighbors within the specified radius.\n",
    "\n",
    "### --> The anomaly score in KNN is generally based on the distance to the k-th nearest neighbor. If k is set to 10 and the data point only has 2 neighbors within a radius of 0.5, then it would not meet the requirement of having at least 10 neighbors. As a result, the KNN algorithm might not produce a meaningful anomaly score for this particular data point using the specified settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3fd383-ac56-4f2d-bd21-eb48288bdb82",
   "metadata": {},
   "source": [
    "# 9.\n",
    "## Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c0a683b-de2a-45ba-9a85-65c5722aced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly score: 0.7407853923164064\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "n_tree=100\n",
    "data_size=3000\n",
    "avg_length=5.0\n",
    "\n",
    "c=np.log2(data_size)\n",
    "anomaly_score=2 ** (-avg_length / c)\n",
    "print(\"Anomaly score:\",anomaly_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11800dd0-6281-4026-b9e2-a1e027837bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
