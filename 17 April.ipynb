{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee0cc08-dede-46db-a658-dfdea9794cea",
   "metadata": {},
   "source": [
    "# 1.\n",
    "## What is Gradient Boosting Regression?\n",
    "### -> Gradient Boosting Regression is a machine learning algorithm used for regression tasks, which involves predicting continuous numerical values. It is an ensemble method that combines multiple weak prediction models, typically decision trees, to create a strong predictive model.\n",
    "### -> The gradient boosting regression algorithm works by iteratively adding weak learners to a growing ensemble. Each new learner is trained to correct the mistakes made by the existing ensemble. This is achieved by fitting the new learner to the residuals, or errors, of the previous ensemble predictions. The algorithm places more emphasis on the data points that were poorly predicted in the previous iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a67287-916a-4416-b268-0a911cb488c4",
   "metadata": {},
   "source": [
    "# 2.\n",
    "## Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb76cea-3f0e-4296-819b-72b869329ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 3738.7286057647766\n",
      "R-squared: 0.5510719051912709\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "\n",
    "X, y = make_regression(n_samples=10, n_features=3, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "reg= GradientBoostingRegressor(n_estimators=10)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error:\",mse)\n",
    "print(f\"R-squared:\",r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f02db6-8bc9-43fb-84b7-330b8a9c6518",
   "metadata": {},
   "source": [
    "# 3.\n",
    "## Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "948cdc4d-02b2-4fe0-8656-9aa25976d080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 100, 'max_depth': 3, 'learning_rate': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "params = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'max_depth': [1,3,5]\n",
    "}\n",
    "clf=GradientBoostingRegressor()\n",
    "random_search=RandomizedSearchCV(clf,param_distributions=params,n_iter=10,cv=5)\n",
    "\n",
    "random_search.fit(X_train,y_train)\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d86c62-0c26-4e5f-aa5e-be2429c36cf5",
   "metadata": {},
   "source": [
    "# 4.\n",
    "## What is a weak learner in Gradient Boosting?\n",
    "### -->In Gradient Boosting, a weak learner refers to a simple or \"weak\" predictive model that is typically used as a base model in the ensemble. Unlike strong or complex models, weak learners are relatively simple and have limited predictive power on their own. However, when combined through the boosting process, they contribute to building a strong predictive model. \n",
    "### -->In the context of Gradient Boosting, the most commonly used weak learner is a decision tree with shallow depth, often referred to as a decision stump. A decision stump is a decision tree with only one split, resulting in two leaf nodes. By using decision stumps as weak learners, Gradient Boosting can capture and learn from the linear relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1f59d3-ee13-44c1-a93d-2cc55b10ccd3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 5.\n",
    "## What is the intuition behind the Gradient Boosting algorithm?\n",
    "### -->The intuition behind the Gradient Boosting algorithm is to iteratively build a strong predictive model by combining multiple weak models in a way that minimizes the overall prediction errors. It leverages the concept of gradients, specifically the negative gradients or residuals, to guide the learning process.\n",
    "### Here's the intuition behind Gradient Boosting:\n",
    "#### 1] Start with an initial model: Initially, a simple model, often referred to as the \"base model\" or \"initial guess,\" is fitted to the training data. This model can be as simple as the mean value of the target variable or a constant value.\n",
    "#### 2] Sequential model building: Gradient Boosting proceeds by iteratively adding weak models to the ensemble. In each iteration, a new weak model is trained to correct the errors or residuals of the ensemble built so far.\n",
    "#### 3] Gradient calculation: The key idea is to calculate the negative gradients or residuals of the ensemble's predictions with respect to the target variable. These gradients represent the direction and magnitude of the errors made by the ensemble.\n",
    "#### 4] Training weak models: The weak models, often decision trees with shallow depth, are trained on the negative gradients or residuals. They learn to approximate the relationship between the features and the negative gradients, effectively capturing the remaining patterns and errors in the data.\n",
    "#### 5] Ensemble update: The predictions of the newly trained weak model are combined with the predictions of the ensemble built so far. The combination is done by adding or subtracting a fraction of the weak model's predictions from the ensemble's predictions. The fraction is determined through a process called learning rate, which controls the contribution of each weak model.\n",
    "#### 6] Iterative learning: Steps 3 to 5 are repeated for a predefined number of iterations or until a stopping criterion is met. Each subsequent weak model is trained to minimize the errors made by the ensemble in the previous iteration.\n",
    "#### 7] Final prediction: The final prediction of the Gradient Boosting algorithm is made by summing the predictions of all weak models in the ensemble, weighted by their individual contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51d2046-c9c7-45fb-b691-522a91cedce1",
   "metadata": {},
   "source": [
    "# 6.\n",
    "## How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "### --> The Gradient Boosting algorithm builds an ensemble of weak learners in a sequential manner. Here's an overview of how Gradient Boosting constructs the ensemble:\n",
    "#### Initialize the ensemble\n",
    "#### Compute the pseudo-residuals\n",
    "#### Train a weak learner\n",
    "#### Update the ensemble\n",
    "#### Update the residuals\n",
    "#### Repeat steps 3 to 5\n",
    "#### Final prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f984b3-a3a0-4da6-914f-abba8e9dfa8c",
   "metadata": {},
   "source": [
    "# 7.\n",
    "## What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "### --> Constructing the mathematical intuition behind the Gradient Boosting algorithm involves several key steps. Here's an overview of the process:\n",
    "#### Define the loss function\n",
    "#### Initialize the model\n",
    "#### Define the residual \n",
    "#### Train a weak learner\n",
    "#### Update the model\n",
    "#### Repeat steps 3 to 5\n",
    "#### Ensemble the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ff4b7-ecab-4e87-985c-51d0f34bb1d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
